{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf4ab03-e30b-49ff-beaf-e5caedee0c94",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m trange\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import SVI, TraceMeanField_ELBO\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8584c3a6-8cd1-4fd6-94b0-26898a674e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = fetch_20newsgroups(subset='all')\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=20, stop_words='english')\n",
    "docs = torch.from_numpy(vectorizer.fit_transform(news['data']).toarray())\n",
    "\n",
    "vocab = pd.DataFrame(columns=['word', 'index'])\n",
    "vocab['word'] = vectorizer.get_feature_names_out()\n",
    "vocab['index'] = vocab.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18b36fae-8458-4041-9987-78710d9343a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary size: 12722\n",
      "Corpus size: torch.Size([18846, 12722])\n"
     ]
    }
   ],
   "source": [
    "print('Dictionary size: %d' % len(vocab))\n",
    "print('Corpus size: {}'.format(docs.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb51ad13-347a-416a-acab-0363b82fc323",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # Base class for the encoder net, used in the guide\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout)  # to avoid component collapse\n",
    "        self.fc1 = nn.Linear(vocab_size, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fcmu = nn.Linear(hidden, num_topics)\n",
    "        self.fclv = nn.Linear(hidden, num_topics)\n",
    "        # NB: here we set `affine=False` to reduce the number of learning parameters\n",
    "        # See https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
    "        # for the effect of this flag in BatchNorm1d\n",
    "        self.bnmu = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "        self.bnlv = nn.BatchNorm1d(num_topics, affine=False)  # to avoid component collapse\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        h = F.softplus(self.fc1(inputs))\n",
    "        h = F.softplus(self.fc2(h))\n",
    "        h = self.drop(h)\n",
    "        # Œº and Œ£ are the outputs\n",
    "        logtheta_loc = self.bnmu(self.fcmu(h))\n",
    "        logtheta_logvar = self.bnlv(self.fclv(h))\n",
    "        logtheta_scale = (0.5 * logtheta_logvar).exp()  # Enforces positivity\n",
    "        return logtheta_loc, logtheta_scale\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    # Base class for the decoder net, used in the model\n",
    "    def __init__(self, vocab_size, num_topics, dropout):\n",
    "        super().__init__()\n",
    "        self.beta = nn.Linear(num_topics, vocab_size, bias=False)\n",
    "        self.bn = nn.BatchNorm1d(vocab_size, affine=False)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        inputs = self.drop(inputs)\n",
    "        # the output is œÉ(Œ≤Œ∏)\n",
    "        return F.softmax(self.bn(self.beta(inputs)), dim=1)\n",
    "\n",
    "\n",
    "class ProdLDA(nn.Module):\n",
    "    def __init__(self, vocab_size, num_topics, hidden, dropout):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_topics = num_topics\n",
    "        self.encoder = Encoder(vocab_size, num_topics, hidden, dropout)\n",
    "        self.decoder = Decoder(vocab_size, num_topics, dropout)\n",
    "\n",
    "    def model(self, docs):\n",
    "        pyro.module(\"decoder\", self.decoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution\n",
    "            logtheta_loc = docs.new_zeros((docs.shape[0], self.num_topics))\n",
    "            logtheta_scale = docs.new_ones((docs.shape[0], self.num_topics))\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "            theta = F.softmax(logtheta, -1)\n",
    "\n",
    "            # conditional distribution of ùë§ùëõ is defined as\n",
    "            # ùë§ùëõ|ùõΩ,ùúÉ ~ Categorical(ùúé(ùõΩùúÉ))\n",
    "            count_param = self.decoder(theta)\n",
    "            # Currently, PyTorch Multinomial requires `total_count` to be homogeneous.\n",
    "            # Because the numbers of words across documents can vary,\n",
    "            # we will use the maximum count accross documents here.\n",
    "            # This does not affect the result because Multinomial.log_prob does\n",
    "            # not require `total_count` to evaluate the log probability.\n",
    "            total_count = int(docs.sum(-1).max())\n",
    "            pyro.sample(\n",
    "                'obs',\n",
    "                dist.Multinomial(total_count, count_param),\n",
    "                obs=docs\n",
    "            )\n",
    "\n",
    "    def guide(self, docs):\n",
    "        pyro.module(\"encoder\", self.encoder)\n",
    "        with pyro.plate(\"documents\", docs.shape[0]):\n",
    "            # Dirichlet prior ùëù(ùúÉ|ùõº) is replaced by a logistic-normal distribution,\n",
    "            # where Œº and Œ£ are the encoder network outputs\n",
    "            logtheta_loc, logtheta_scale = self.encoder(docs)\n",
    "            logtheta = pyro.sample(\n",
    "                \"logtheta\", dist.Normal(logtheta_loc, logtheta_scale).to_event(1))\n",
    "\n",
    "    def beta(self):\n",
    "        # beta matrix elements are the weights of the FC layer on the decoder\n",
    "        return self.decoder.beta.weight.cpu().detach().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4c7c6a1-dbf9-4da5-b125-0436670fd8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting global variables\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "pyro.set_rng_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_topics = 20 \n",
    "docs = docs.float().to(device)\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5da56faf-6f14-472c-8a0a-d8cf98250320",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [04:37<00:00,  5.56s/it, epoch_loss=3.72e+05]\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "pyro.clear_param_store()\n",
    "\n",
    "prodLDA = ProdLDA(\n",
    "    vocab_size=docs.shape[1],\n",
    "    num_topics=num_topics,\n",
    "    hidden=100,\n",
    "    dropout=0.2\n",
    ")\n",
    "prodLDA.to(device)\n",
    "\n",
    "optimizer = pyro.optim.Adam({\"lr\": learning_rate})\n",
    "svi = SVI(prodLDA.model, prodLDA.guide, optimizer, loss=TraceMeanField_ELBO())\n",
    "num_batches = int(math.ceil(docs.shape[0] / batch_size)) \n",
    "\n",
    "bar = trange(num_epochs)\n",
    "for epoch in bar:\n",
    "    running_loss = 0.0\n",
    "    for i in range(num_batches):\n",
    "        batch_docs = docs[i * batch_size:(i + 1) * batch_size, :]\n",
    "        loss = svi.step(batch_docs)\n",
    "        running_loss += loss / batch_docs.size(0)\n",
    "\n",
    "    bar.set_postfix(epoch_loss='{:.2e}'.format(running_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f154ac0f-beea-4fa6-ae50-bf93ab9ef328",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m     18\u001b[0m     beta \u001b[38;5;241m=\u001b[39m prodLDA\u001b[38;5;241m.\u001b[39mbeta()\n\u001b[1;32m     19\u001b[0m     fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m7\u001b[39m, \u001b[38;5;241m3\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m14\u001b[39m, \u001b[38;5;241m24\u001b[39m))\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "def plot_word_cloud(b, ax, v, n):\n",
    "    sorted_, indices = torch.sort(b, descending=True)\n",
    "    df = pd.DataFrame(indices[:100].numpy(), columns=['index'])\n",
    "    words = pd.merge(df, vocab[['index', 'word']],\n",
    "                     how='left', on='index')['word'].values.tolist()\n",
    "    sizes = (sorted_[:100] * 1000).int().numpy().tolist()\n",
    "    freqs = {words[i]: sizes[i] for i in range(len(words))}\n",
    "    wc = WordCloud(background_color=\"white\", width=800, height=500)\n",
    "    wc = wc.generate_from_frequencies(freqs)\n",
    "    ax.set_title('Topic %d' % (n + 1))\n",
    "    ax.imshow(wc, interpolation='bilinear')\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "if not False:\n",
    "    import matplotlib.pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "\n",
    "    beta = prodLDA.beta()\n",
    "    fig, axs = plt.subplots(7, 3, figsize=(14, 24))\n",
    "    for n in range(beta.shape[0]):\n",
    "        i, j = divmod(n, 3)\n",
    "        plot_word_cloud(beta[n], axs[i, j], vocab, n)\n",
    "    axs[-1, -1].axis('off');\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyro",
   "language": "python",
   "name": "pyro"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
